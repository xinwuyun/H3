{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back on slow Vandermonde kernel. Install pykeops for improved memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "from train import load_openwebtext\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [47562, 9107, 9019], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"wangzerui\", truncation=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset openwebtext (/cpfs01/user/wangzerui/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f5dc6053044df7a2f87905e88843bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/regex/regex.py\", line 489, in _compile\n",
      "    args_needed = _named_args[args_key]\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "KeyError: (regex.Regex(\"'s|'t|'re|'ve|'m|'ll|'d| ?\\\\p{L}+| ?\\\\p{N}+| ?[^\\\\s\\\\p{L}\\\\p{N}]+|\\\\s+(?!\\\\S)|\\\\s+\", flags=regex.V0), <class '_regex.Pattern'>, 0)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "Process ForkPoolWorker-14:\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "Process ForkPoolWorker-13:\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "Traceback (most recent call last):\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 701, in get_input_ids\n",
      "    return self.convert_tokens_to_ids(tokens)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 517, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 579, in convert_tokens_to_ids\n",
      "    ids.append(self._convert_token_to_id_with_added_voc(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 151, in split\n",
      "    for start, trie_pointer in states.items():\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 517, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 701, in get_input_ids\n",
      "    return self.convert_tokens_to_ids(tokens)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 301, in _tokenize\n",
      "    for token in re.findall(self.pat, text):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 701, in get_input_ids\n",
      "    return self.convert_tokens_to_ids(tokens)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 151, in split\n",
      "    for start, trie_pointer in states.items():\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 212, in bpe\n",
      "    if token in self.cache:\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 579, in convert_tokens_to_ids\n",
      "    ids.append(self._convert_token_to_id_with_added_voc(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3480, in _map_single\n",
      "    writer.write_batch(batch)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 302, in _tokenize\n",
      "    token = \"\".join(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3463, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/regex/regex.py\", line 338, in findall\n",
      "    return pat.findall(string, pos, endpos, overlapped, concurrent, timeout)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 579, in convert_tokens_to_ids\n",
      "    ids.append(self._convert_token_to_id_with_added_voc(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 212, in bpe\n",
      "    if token in self.cache:\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 588, in _convert_token_to_id_with_added_voc\n",
      "    return self._convert_token_to_id(token)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 217, in bpe\n",
      "    if not pairs:\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 554, in write_batch\n",
      "    inferred_features[col] = typed_sequence.get_inferred_type()\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 303, in <genexpr>\n",
      "    self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3344, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 212, in bpe\n",
      "    if token in self.cache:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 310, in _convert_token_to_id\n",
      "    return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 129, in get_inferred_type\n",
      "    self._inferred_type = generate_from_arrow_type(pa.array(self).type)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 302, in _tokenize\n",
      "    token = \"\".join(\n",
      "  File \"/cpfs01/user/wangzerui/H3/train.py\", line 44, in encode_examples\n",
      "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=seq_length)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "KeyboardInterrupt\n",
      "  File \"pyarrow/array.pxi\", line 243, in pyarrow.lib.array\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 303, in <genexpr>\n",
      "    self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2561, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2647, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 189, in __arrow_array__\n",
      "    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2838, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 301, in _tokenize\n",
      "    for token in re.findall(self.pat, text):\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 733, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/regex/regex.py\", line 337, in findall\n",
      "    pat = _compile(pattern, flags, ignore_unused, kwargs, True)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 700, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/regex/regex.py\", line 489, in _compile\n",
      "    args_needed = _named_args[args_key]\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 547, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 305, in _tokenize\n",
      "    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
      "  File \"/opt/conda/envs/H3/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py\", line 212, in bpe\n",
      "    if token in self.cache:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py:1348\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/managers.py:835\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    834\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 835\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/connection.py:253\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 253\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/connection.py:417\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 417\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/connection.py:382\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_openwebtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataloader)\n",
      "File \u001b[0;32m~/H3/train.py:48\u001b[0m, in \u001b[0;36mload_openwebtext\u001b[0;34m(tokenizer, seq_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_openwebtext\u001b[39m(tokenizer, seq_length):\n\u001b[1;32m     47\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenwebtext/openwebtext.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/arrow_dataset.py:3180\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3172\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3173\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3174\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3175\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3178\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3179\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3181\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3182\u001b[0m     ):\n\u001b[1;32m   3183\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3184\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py:1354\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/datasets/utils/py_utils.py:1354\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m     [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/opt/conda/envs/H3/lib/python3.8/site-packages/multiprocess/pool.py:767\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_success:\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import load_openwebtext\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_openwebtext(tokenizer, 1024)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6cf200f1a1b4bcd81e785bda5548e50342366c60a42b99499c85db63a235865"
  },
  "kernelspec": {
   "display_name": "H3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
